{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ZCbXjrQYHhAqCfn0W06cCVeBk82lL5_D",
      "authorship_tag": "ABX9TyPOLlCv01d2RRz9oR+jg1xM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seirah-yang/F_roject/blob/main/%EB%AC%B8%EC%84%9C%EC%83%9D%EC%84%B1%EA%B8%B0_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.40.0 sentence-transformers==2.2.2 torch==2.3.0 tqdm python-docx PyPDF2\n"
      ],
      "metadata": {
        "id": "Ztiga9bezZ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/reference_file"
      ],
      "metadata": {
        "id": "Gk685n_Uzj0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers==2.2.2\n",
        "!pip install -U huggingface_hub==0.24.6\n",
        "!pip install torch==2.3.0 transformers==4.40.0 tqdm python-docx PyPDF2"
      ],
      "metadata": {
        "id": "7Qdfv2mx8jRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "Iv3bGG6uzb63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRBbGlzayAoz"
      },
      "outputs": [],
      "source": [
        "#/* RND_Plan_Pipeline_v2_Stable_fixTokenizer.py */\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "(A) ContextSearch\n",
        "        â””â”€ e5-large ê¸°ë°˜ ê·¼ê±° ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ë°˜ì˜)\n",
        "(B) DraftGeneration\n",
        "        â””â”€ A.X 4.0 Lightìœ¼ë¡œ ì´ˆì•ˆ ìƒì„±\n",
        "(C) Validation\n",
        "        â”œâ”€ Rule-based ê²€ì‚¬\n",
        "        â””â”€ NLI entailment ì ìˆ˜ ê³„ì‚°\n",
        "(D) Export Report\n",
        "        â””â”€ DOCX ì €ì¥ + ê²€ì¦ë¦¬í¬íŠ¸ + ê·¼ê±°ë¬¸ì„œëª©ë¡\n",
        "\"\"\"\n",
        "\n",
        "import os, re, json, glob, torch\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, pipeline,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# ==============================\n",
        "# 1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "# =============================\n",
        "LAW_DIR = \"/content/drive/MyDrive/reference_file\"\n",
        "\n",
        "E5_NAME = \"intfloat/multilingual-e5-large\"\n",
        "GEN_NAME = \"skt/A.X-4.0-Light\"\n",
        "NLI_MODEL = \"joeddav/xlm-roberta-large-xnli\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# GPU ë©”ëª¨ë¦¬ ê°ì§€ â†’ ì•ˆì „ í† í° ì„¤ì •\n",
        "GEN_MAX_NEW_TOKENS = 1500\n",
        "if torch.cuda.is_available():\n",
        "    free, total = torch.cuda.mem_get_info()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "GEN_DO_SAMPLE = False\n",
        "\n",
        "# ==============================\n",
        "# 2. ìœ í‹¸ë¦¬í‹°\n",
        "# ==============================\n",
        "def first_n_lines(text: str, n_chars=400):\n",
        "    t = \" \".join(str(text).split())\n",
        "    return t[:n_chars]\n",
        "\n",
        "def clean_generated_text(text: str) -> str:\n",
        "    text = re.sub(r'[â€¢â—â–ªâ–¶â—‡â—†â–¡â–ªï¸â–«ï¸â€“]', ' ', text)\n",
        "    text = re.sub(r'^\\s*[-#*]+\\s*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    text = re.sub(r'\\s+([\\.,;:])', r'\\1', text)\n",
        "    return text.strip()\n",
        "\n",
        "def has_number(text: str):\n",
        "    return bool(re.search(r\"\\d\", text))\n",
        "\n",
        "# ==============================\n",
        "# 3. ì„¹ì…˜ ì •ì˜ (ì˜ˆì‹œ 3ê°œë§Œ)\n",
        "# ==============================\n",
        "sections = [\n",
        "    {\"section\": \"ì—°êµ¬ê¸°íšê³¼ì œì˜ ê°œìš”\",\n",
        "        \"role\": \"ì œì•ˆì„œ ì´ê´„ ì—ë””í„°\",\n",
        "        \"query\": \"ê³¼ì œì˜ ëª©ì , í•„ìš”ì„±, ê¸°ëŒ€íš¨ê³¼ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ 500ì ë‚´ì™¸ë¡œ ëª…ë£Œí•˜ê²Œ ìš”ì•½í•¨. í‰ê°€ìê°€ ê³¼ì œì˜ ì „ì²´ êµ¬ì¡°ë¥¼ í•œëˆˆì— ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê¸°ìˆ í•¨.\",\n",
        "        \"constraints\": [\"500ì ë‚´ì™¸\", \"ë…¼ë¦¬ì  ì—°ê²° êµ¬ì¡° ìœ ì§€\", \"í•µì‹¬ ìš”ì•½ ì¤‘ì‹¬\"],\n",
        "        \"output_format\": \"ì„œìˆ ë¬¸\"},\n",
        "    {\"section\": \"ì—°êµ¬ê°œë°œê³¼ì œì˜ ë°°ê²½\",\n",
        "        \"role\": \"R&D ê¸°íš ì „ë¬¸ê°€\",\n",
        "        \"query\": \"ì œê³µëœ ì„ í–‰ì—°êµ¬, ì‹œì¥ ë™í–¥, ì •ì±…ìë£Œë¥¼ ê·¼ê±°ë¡œ ë³¸ ê³¼ì œê°€ ì¶”ì§„ë˜ì–´ì•¼ í•˜ëŠ” ë‹¹ìœ„ì„±ê³¼ RFP(ê³µê³ ë¬¸) ë¶€í•©ì„±ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì œì‹œí•¨.\",\n",
        "        \"constraints\": [\"ë°ì´í„° ê·¼ê±° í¬í•¨\", \"RFP ë¬¸í•­ ë¶€í•©ì„± ëª…ì‹œ\"],\n",
        "        \"output_format\": \"ì„œìˆ ë¬¸\"},\n",
        "    {\"section\": \"ì—°êµ¬ê°œë°œê³¼ì œì˜ í•„ìš”ì„±\",\n",
        "        \"role\": \"ì‚°ì—…ë¶„ì„ê°€\",\n",
        "        \"query\": \"ë°ì´í„°ì™€ ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ í˜„ì¬ ê¸°ìˆ ì Â·ì‚°ì—…ì  ë¬¸ì œì ì„ ì œì‹œí•˜ê³ , ë³¸ ê³¼ì œê°€ ì´ë¥¼ í•´ê²°í•´ì•¼ í•˜ëŠ” í•„ìš”ì„±ì„ ì¸ê³¼ì ìœ¼ë¡œ ì„œìˆ í•¨.\",\n",
        "        \"constraints\": [\"ì¸ê³¼ê´€ê³„ êµ¬ì¡°\", \"ë°ì´í„° ê·¼ê±° ì œì‹œ\"],\n",
        "        \"output_format\": \"ì„œìˆ ë¬¸\"},\n",
        "    {\"section\": \"ê¸°ìˆ ê°œë°œ í•µì‹¬ì–´(í‚¤ì›Œë“œ)\",\n",
        "        \"role\": \"ê¸°ìˆ  ë„¤ì´ë° ì „ëµê°€\",\n",
        "        \"query\": \"ê³¼ì œì˜ ì •ì²´ì„±ê³¼ í•µì‹¬ ê¸°ìˆ ì„ ëŒ€í‘œí•˜ëŠ” í‚¤ì›Œë“œ 5ê°œë¥¼ êµ­ë¬¸Â·ì˜ë¬¸ ê³µì‹ ëª…ì¹­ìœ¼ë¡œ ì œì‹œí•¨. ê° ìš©ì–´ëŠ” êµ­ì œ í‘œì¤€ ë˜ëŠ” í•™ìˆ  ì •ì˜ë¥¼ ê·¼ê±°ë¡œ í•˜ë©°, ì„ ì • ì‚¬ìœ ë¥¼ ê°„ë‹¨íˆ ëª…ì‹œí•¨.\",\n",
        "        \"constraints\": [\"êµ­ë¬¸Â·ì˜ë¬¸ ë³‘ê¸°\", \"êµ­ì œ í‘œì¤€ ê¸°ë°˜ ì •ì˜ í¬í•¨\", \"5ê°œ ì´ë‚´\"],\n",
        "        \"output_format\": \"í‘œ(í‚¤ì›Œë“œ/ì˜ë¬¸ëª…/ì •ì˜/ì¶œì²˜)\"},\n",
        "    {\"section\": \"ì—°êµ¬ê°œë°œ ëª©í‘œ\",\n",
        "        \"role\": \"R&D PMO\",\n",
        "        \"query\": \"ê³¼ì œì˜ ìµœì¢… ëª©í‘œë¥¼ 500ì ë‚´ì™¸ë¡œ ëª…í™•íˆ ê¸°ìˆ í•¨. í•µì‹¬ ì„±ëŠ¥ì§€í‘œ(KPI), ë‹¬ì„± ê¸°ì¤€(ìˆ˜ì¹˜Â·ë‹¨ìœ„Â·ë§ˆì¼ìŠ¤í†¤), ê²€ì¦ ë°©ë²•ì„ í¬í•¨í•˜ë©°, ëª¨í˜¸í•œ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.\",\n",
        "        \"constraints\": [\"ì •ëŸ‰í™”ëœ ìˆ˜ì¹˜ í¬í•¨\", \"KPI ë° ê²€ì¦ë°©ë²• ëª…ì‹œ\"],\n",
        "        \"output_format\": \"ì„œìˆ ë¬¸ + í‘œ(KPI/ë‹¨ìœ„/ê¸°ì¤€/ê²€ì¦ë°©ë²•)\"},\n",
        "        {\"section\": \"ì—°êµ¬ê°œë°œ ë‚´ìš©\",\n",
        "            \"role\": \"ê¸°ìˆ ì´ê´„ì(Tech Lead)\",\n",
        "            \"query\": \"ì „ì²´ ì—°êµ¬ ë²”ìœ„ë¥¼ 1,000ì ë‚´ì™¸ë¡œ ì²´ê³„ì ìœ¼ë¡œ ê¸°ìˆ í•¨. í•µì‹¬ ê¸°ìˆ ìš”ì†Œ, ì„¸ë¶€ ê³¼ì œ êµ¬ì¡°, ë°ì´í„° ë° ì‹œìŠ¤í…œ íë¦„, ì„±ëŠ¥ í‰ê°€ ê³„íšì„ ëª…ì‹œí•¨.\",\n",
        "            \"constraints\": [\"1,000ì ë‚´ì™¸\", \"ê¸°ìˆ ìš”ì†Œ ë° í‰ê°€ê³„íš í¬í•¨\"],\n",
        "            \"output_format\": \"ì„œìˆ ë¬¸ + ë„ì‹(ê¸°ìˆ íë¦„)\"},\n",
        "        {\"section\": \"ì—°ì°¨ë³„ ê°œë°œëª©í‘œ\",\n",
        "            \"role\": \"PMO ë¦¬ë”\",\n",
        "            \"query\": \"ìµœì¢… ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ì—°ì°¨ë³„ ë° ê¸°ê´€ë³„ ê°œë°œ ëª©í‘œë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì œì‹œí•¨. ê° ì—°ì°¨ë³„ KPI, ë§ˆì¼ìŠ¤í†¤, ê²€ì¦ ë°©ë²•ì„ ëª…í™•íˆ í¬í•¨í•¨.\",\n",
        "            \"constraints\": [\"ì—°ì°¨ë³„ êµ¬ë¶„\", \"ì •ëŸ‰ì  ì§€í‘œ í¬í•¨\"],\n",
        "            \"output_format\": \"í‘œ(ì—°ì°¨/KPI/ë§ˆì¼ìŠ¤í†¤/ê²€ì¦ë°©ë²•)\"},\n",
        "        {\"section\": \"ì—°ì°¨ë³„ ê°œë°œë‚´ìš© ë° ë²”ìœ„\",\n",
        "        \"role\": \"ê³µë™ì—°êµ¬ ì»¨ì†Œì‹œì—„ ì½”ë””ë„¤ì´í„°\",\n",
        "        \"query\": \"ì°¸ì—¬ ê¸°ê´€ë³„ ì—­í• , ì±…ì„, ì—°ì°¨ë³„ ì‚°ì¶œë¬¼ì„ ì¤‘ë³µ ë° ëˆ„ë½ ì—†ì´ ê¸°ìˆ í•¨. ê³µë™ê¸°ê´€ì´ ì—†ëŠ” ê²½ìš° í•´ë‹¹ í•­ëª©ì€ ìƒëµí•¨.\",\n",
        "        \"constraints\": [\"ê¸°ê´€ë³„ ì—­í•  ëª…í™•í™”\", \"ì¤‘ë³µ/ëˆ„ë½ ê¸ˆì§€\"],\n",
        "        \"output_format\": \"í‘œ(ê¸°ê´€/ì—­í• /ì‚°ì¶œë¬¼/ì±…ì„)\"},\n",
        "        {\"section\": \"ì¶”ì§„ë°©ë²• ë° ì „ëµ\",\n",
        "            \"role\": \"ì´ê´„ ì•„í‚¤í…íŠ¸(Chief Architect)\",\n",
        "            \"query\": \"í•µì‹¬ ê¸°ìˆ ê°œë°œ ë°©ë²•ë¡ , ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ë°©ì•ˆ, ì„±ëŠ¥ ê²€ì¦ ê³„íšì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì œì‹œí•¨. ê¸°ìˆ ì˜ ìš°ìˆ˜ì„±ê³¼ ì‹¤í˜„ ê°€ëŠ¥ì„±ì„ ì…ì¦í•¨.\",\n",
        "            \"constraints\": [\"í•µì‹¬ ë°©ë²•ë¡  í¬í•¨\", \"ë¦¬ìŠ¤í¬ ë° ê²€ì¦ê³„íš ëª…ì‹œ\"],\n",
        "            \"output_format\": \"ì„œìˆ ë¬¸ + í‘œ(ë¦¬ìŠ¤í¬/ëŒ€ì‘ë°©ì•ˆ)\"},\n",
        "        {\"section\": \"ê³¼ì œ ì„±ê³¼ì˜ í™œìš©ë°©ì•ˆ\",\n",
        "            \"role\": \"ì‚¬ì—…ê°œë°œ ì´ê´„(BD Head)\",\n",
        "            \"query\": \"ì—°êµ¬ ì„±ê³¼ê°€ ì‹¤ì œ ì‚°ì—… ë° ì‹œì¥ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆëŠ”ì§€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•¨. ëª©í‘œ ì‹œì¥, ì£¼ìš” ìˆ˜ìš”ì²˜, í•µì‹¬ ì ìš© ì‹œë‚˜ë¦¬ì˜¤, ê¸°ìˆ ì˜ ì°¨ë³„í™”ëœ ê°€ì¹˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‚¬ì—…í™” ë°©í–¥ì„ ì„¤ëª…í•¨.\",\n",
        "            \"constraints\": [\"ì‹œì¥ ì‹œë‚˜ë¦¬ì˜¤ í¬í•¨\", \"Value Proposition ì¤‘ì‹¬\"],\n",
        "            \"output_format\": \"ì„œìˆ ë¬¸ + í‘œ(ì‹œì¥/ìˆ˜ìš”ì²˜/ì ìš©ì‹œë‚˜ë¦¬ì˜¤)\"},\n",
        "        {\"section\": \"ì‹ ê·œì‚¬ì—… ì‹ ì„¤ì˜ ê¸°ëŒ€íš¨ê³¼\",\n",
        "            \"role\": \"ê±°ì‹œê²½ì œ ë¶„ì„ê°€(Macro-Economic Analyst)\",\n",
        "            \"query\": \"ë³¸ ê³¼ì œê°€ êµ­ê°€ ê²½ì œì— ë¯¸ì¹˜ëŠ” íŒŒê¸‰íš¨ê³¼ë¥¼ ì •ëŸ‰ì  ì§€í‘œë¡œ ì œì‹œí•¨. ì‹œì¥ ì°½ì¶œ, ìˆ˜ì… ëŒ€ì²´, ìˆ˜ì¶œ ì¦ëŒ€, ì¼ìë¦¬ ì°½ì¶œ ë“± ê±°ì‹œì  íš¨ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ ì¦ëª…í•¨.\",\n",
        "            \"constraints\": [\"ì •ëŸ‰ì  ìˆ˜ì¹˜ ê¸°ë°˜\", \"ê²½ì œíš¨ê³¼ ëª…ì‹œ\"],\n",
        "            \"output_format\": \"í‘œ(ì§€í‘œ/ì˜ˆìƒê°’/ê·¼ê±°ìë£Œ)\"},\n",
        "        {\"section\": \"ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ ê³„íš\",\n",
        "            \"role\": \"ì‚¬íšŒì  ê°€ì¹˜ ì „ëµê°€(Social Value Strategist)\",\n",
        "            \"query\": \"ê³¼ì œì˜ ì‚¬íšŒì  ë¹„ì „ê³¼ ëª©í‘œë¥¼ ì •ì˜í•˜ê³ , ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë¡œë“œë§µì„ ìˆ˜ë¦½í•¨. ë³´ê±´, í™˜ê²½, ì•ˆì „ ë“± ì‚¬íšŒì  ê°€ì¹˜ ë²”ì£¼ì™€ ì—°ê³„í•¨.\",\n",
        "            \"constraints\": [\"ì‚¬íšŒì  ê°€ì¹˜ ë²”ì£¼ ëª…ì‹œ\", \"ë¡œë“œë§µ í¬í•¨\"],\n",
        "            \"output_format\": \"ì„œìˆ ë¬¸ + í‘œ(ëª©í‘œ/ì‹¤í–‰ë‹¨ê³„/ì„±ê³¼ì§€í‘œ)\"},\n",
        "        {\"section\": \"ì‚¬íšŒì  ê°€ì¹˜ì°½ì¶œì˜ ê¸°ëŒ€íš¨ê³¼\",\n",
        "            \"role\": \"ì„íŒ©íŠ¸ í‰ê°€ ì „ë¬¸ê°€(Impact Assessor)\",\n",
        "            \"query\": \"ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ ê³„íšì´ ì‹¤í–‰ë˜ì—ˆì„ ë•Œ ì˜ˆìƒë˜ëŠ” ê¸ì •ì  ë³€í™”ë¥¼ ì •ëŸ‰ì  ë° ì •ì„±ì  ì„íŒ©íŠ¸ ì§€í‘œë¡œ ì œì‹œí•¨. ì‚¬íšŒì  íŒŒê¸‰íš¨ê³¼ë¥¼ ê°ê´€ì ìœ¼ë¡œ ì„¤ëª…í•¨.\",\n",
        "            \"constraints\": [\"ì •ëŸ‰/ì •ì„± ì§€í‘œ ë³‘ê¸°\", \"ì‚¬íšŒì  íŒŒê¸‰íš¨ê³¼ í¬í•¨\"],\n",
        "            \"output_format\": \"í‘œ(ì§€í‘œìœ í˜•/ì„±ê³¼/ì¸¡ì •ë°©ë²•)\"},\n",
        "        {\"section\": \"ê²½ì œì  ì„±ê³¼ì°½ì¶œì˜ ê¸°ëŒ€íš¨ê³¼\",\n",
        "            \"role\": \"ìµœê³ ì¬ë¬´ì±…ì„ì(CFO)\",\n",
        "            \"query\": \"ê¸°ì—… ê´€ì ì—ì„œ ë³¸ ê³¼ì œì˜ ì¬ë¬´ì  ì„±ê³¼ë¥¼ êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì œì‹œí•¨. ì˜ˆìƒ ë§¤ì¶œ, ì´ìµ, íˆ¬ììˆ˜ìµë¥ (ROI), ìˆœí˜„ì¬ê°€ì¹˜(NPV) ë“± ì£¼ìš” ì§€í‘œë¥¼ ê·¼ê±°ì™€ í•¨ê»˜ ëª…ë£Œí•˜ê²Œ ê¸°ìˆ í•¨.\",\n",
        "            \"constraints\": [\"ì¬ë¬´ì§€í‘œ í¬í•¨\", \"ì‚°ì¶œê·¼ê±° ëª…ì‹œ\"],\n",
        "            \"output_format\": \"í‘œ(ì§€í‘œ/ì˜ˆìƒê°’/ê·¼ê±°)\"},\n",
        "        {\"section\": \"ì‹ ê·œ ì¸ë ¥ ì±„ìš© ê³„íš ë° í™œìš© ë°©ì•ˆ\",\n",
        "            \"role\": \"ì „ëµì  ì¸ì‚¬ íŒŒíŠ¸ë„ˆ(Strategic HR Partner)\",\n",
        "            \"query\": \"ê³¼ì œ ìˆ˜í–‰ì— í•„ìš”í•œ í•µì‹¬ ì¸ë ¥ì˜ ì±„ìš©, ë°°ì¹˜, êµìœ¡ ê³„íšì„ íƒ€ì„ë¼ì¸ê³¼ í•¨ê»˜ ì œì‹œí•¨. ì¸ë ¥ í™•ë³´ ë° ì—­ëŸ‰ ê·¹ëŒ€í™” ë°©ì•ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•¨.\",\n",
        "            \"constraints\": [\"íƒ€ì„ë¼ì¸ í¬í•¨\", \"ì—­ëŸ‰ ê°•í™” ê³„íš ëª…ì‹œ\"],\n",
        "            \"output_format\": \"í‘œ(ì§ë¬´/ì±„ìš©ì‹œì /êµìœ¡ê³„íš)\"},\n",
        "        {\"section\": \"ë³´ì•ˆë“±ê¸‰ì˜ ë¶„ë¥˜ ë° í•´ë‹¹ ì‚¬ìœ \",\n",
        "            \"role\": \"ë³´ì•ˆê´€ë¦¬ ì±…ì„ì(Security Manager)\",\n",
        "            \"query\": \"ê´€ë ¨ ë²•ë ¹ ë° ë³´ì•ˆê´€ë¦¬ìš”ë ¹ì„ ê·¼ê±°ë¡œ ë³¸ ê³¼ì œì˜ ë³´ì•ˆë“±ê¸‰ì„ ë¶„ë¥˜í•˜ê³ , ê·¸ ê²°ì • ì‚¬ìœ ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ê¸°ìˆ í•¨.\",\n",
        "            \"constraints\": [\"ë²•ë ¹ ê·¼ê±° í¬í•¨\", \"ì‚¬ìœ  ëª…ì‹œ\"],\n",
        "            \"output_format\": \"ì„œìˆ ë¬¸\"}\n",
        "        ]\n",
        "\n",
        "# ==============================\n",
        "# 4. ì„ë² ë”© ê¸°ë°˜ RAG ì¸ë±ìŠ¤ êµ¬ì¶•\n",
        "# ==============================\n",
        "CHUNK_MAX = 500\n",
        "CHUNK_OVERLAP = 50\n",
        "TOPK = 5\n",
        "REF_WEIGHTS = {\n",
        "    \"í–‰ì •ì—…ë¬´ì˜ ìš´ì˜ ë° í˜ì‹ ì— ê´€í•œ ê·œì •(ëŒ€í†µë ¹ë ¹)\": 0.20,\n",
        "    \"í–‰ì •ì—…ë¬´ì˜ ìš´ì˜ ë° í˜ì‹ ì— ê´€í•œ ê·œì • ì‹œí–‰ê·œì¹™\": 0.20,\n",
        "    \"êµ­ê°€ì—°êµ¬ê°œë°œì‚¬ì—… ì—°êµ¬ê°œë°œê³„íšì„œ\": 0.20,\n",
        "    \"ì‚°ì—…ê¸°ìˆ  R&D ê³¼ì œëª… ì‘ì„± ê°€ì´ë“œë¼ì¸\":0.20,\n",
        "    \"ì „ëµê³„íšì„œ ì‘ì„±ì•ˆë‚´ì„œ\": 0.10,\n",
        "    \"Vertical\": 0.10\n",
        "}\n",
        "DEFAULT_WEIGHT = 0.05\n",
        "\n",
        "def guess_weight(filename: str) -> float:\n",
        "    for k, w in REF_WEIGHTS.items():\n",
        "        if k.lower() in filename.lower():\n",
        "            return w\n",
        "    return DEFAULT_WEIGHT\n",
        "\n",
        "def chunk_text(txt: str, max_chars=CHUNK_MAX, overlap=CHUNK_OVERLAP):\n",
        "    txt = \" \".join(str(txt).split())\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(txt):\n",
        "        j = min(len(txt), i + max_chars)\n",
        "        chunks.append(txt[i:j])\n",
        "        if j == len(txt): break\n",
        "        i = max(0, j - overlap)\n",
        "    return chunks\n",
        "\n",
        "def load_reference_chunks(law_dir: str):\n",
        "    items = []\n",
        "    for p in glob.glob(os.path.join(law_dir, \"*\")):\n",
        "        text = \"\"\n",
        "        try:\n",
        "            ext = p.split(\".\")[-1].lower()\n",
        "            if ext == \"pdf\":\n",
        "                reader = PdfReader(p)\n",
        "                for pg in reader.pages:\n",
        "                    text += (pg.extract_text() or \"\") + \"\\n\"\n",
        "            elif ext in (\"docx\", \"doc\"):\n",
        "                d = Document(p)\n",
        "                text = \"\\n\".join([x.text for x in d.paragraphs])\n",
        "            elif ext in (\"rtf\", \"txt\"):\n",
        "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    text = f.read()\n",
        "        except Exception:\n",
        "            continue\n",
        "        base = os.path.basename(p)\n",
        "        weight = guess_weight(base)\n",
        "        for idx, ck in enumerate(chunk_text(text)):\n",
        "            items.append({\"source\": base, \"chunk_id\": idx, \"weight\": weight, \"text\": ck})\n",
        "    return items\n",
        "\n",
        "embed_model = SentenceTransformer(E5_NAME, device=DEVICE)\n",
        "REF_ITEMS = load_reference_chunks(LAW_DIR)\n",
        "REF_EMBS = embed_model.encode(\n",
        "    [it[\"text\"] for it in REF_ITEMS],\n",
        "    convert_to_tensor=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "print(f\"[INFO] Loaded {len(REF_ITEMS)} reference chunks.\")\n",
        "\n",
        "def search_reference(query: str, topk: int = TOPK):\n",
        "    q = embed_model.encode(query, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    sims = util.cos_sim(q, REF_EMBS)[0]\n",
        "    scores = [(float(s) * (1.0 + REF_ITEMS[i][\"weight\"]), i) for i, s in enumerate(sims)]\n",
        "    scores.sort(key=lambda x: x[0], reverse=True)\n",
        "    picks = []\n",
        "    for sc, idx in scores[:topk]:\n",
        "        it = REF_ITEMS[idx]\n",
        "        picks.append({\n",
        "            \"source\": it[\"source\"], \"chunk_id\": it[\"chunk_id\"],\n",
        "            \"weight\": it[\"weight\"], \"score\": round(sc, 4),\n",
        "            \"snippet\": first_n_lines(it[\"text\"], 400)\n",
        "        })\n",
        "    return picks\n",
        "\n",
        "def format_ctx_block(refs):\n",
        "    return \"\\n\".join([\n",
        "        f\"- [{r['source']} | w={r['weight']:.2f} | score={r['score']:.3f}]\\n  {r['snippet']}\"\n",
        "        for r in refs\n",
        "    ])\n",
        "\n",
        "# ==============================\n",
        "# 5. ìƒì„±ëª¨ë¸ ë¡œë“œ\n",
        "# ==============================\n",
        "gen_tok = AutoTokenizer.from_pretrained(\n",
        "    GEN_NAME,\n",
        "    use_fast=False,\n",
        "    trust_remote_code=True,\n",
        "    padding_side='left'   # âœ… ì¶”ê°€\n",
        ")\n",
        "\n",
        "if gen_tok.pad_token_id is None:\n",
        "    gen_tok.pad_token = gen_tok.eos_token\n",
        "\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    GEN_NAME,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ").eval()\n",
        "\n",
        "gen_pipe = pipeline(\"text-generation\", model=gen_model, tokenizer=gen_tok)\n",
        "\n",
        "# ==============================\n",
        "# 6. ê²€ì¦ëª¨ë¸(NLI)\n",
        "# ==============================\n",
        "nli_tok = AutoTokenizer.from_pretrained(NLI_MODEL)\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL).to(DEVICE).eval()\n",
        "\n",
        "def nli_entail_vs_contra(premise, hypothesis):\n",
        "    inputs = nli_tok(premise, hypothesis, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits = nli_model(**inputs).logits\n",
        "    probs = torch.softmax(logits, dim=-1).squeeze(0).tolist()\n",
        "    entail, neutral, contra = probs[0], probs[1], probs[2]\n",
        "    return entail, contra\n",
        "\n",
        "def validate_output(section_obj, generated_text: str, refs):\n",
        "    report = []\n",
        "    cleaned = generated_text.strip()\n",
        "    maxlen = 10**9\n",
        "    for c in section_obj.get(\"constraints\", []):\n",
        "        m = re.search(r\"(\\d+)\\s*ì\", c)\n",
        "        if m:\n",
        "            maxlen = int(m.group(1)) + int(0.3*int(m.group(1)))\n",
        "            break\n",
        "    if len(cleaned) > maxlen:\n",
        "        report.append(f\" ê¸¸ì´ ì´ˆê³¼: {len(cleaned)}ì > í—ˆìš© {maxlen}ì\")\n",
        "    if any(k in section_obj[\"section\"] for k in [\"ëª©í‘œ\", \"KPI\"]) or \"ì •ëŸ‰\" in \" \".join(section_obj.get(\"constraints\", [])):\n",
        "        if not has_number(cleaned):\n",
        "            report.append(\"ì •ëŸ‰ ì§€í‘œ(ìˆ«ì) ë¯¸í¬í•¨\")\n",
        "    entail_sum, contra_sum = 0.0, 0.0\n",
        "    for r in refs[:3]:\n",
        "        e, c = nli_entail_vs_contra(r[\"snippet\"], cleaned)\n",
        "        entail_sum += e; contra_sum += c\n",
        "    score = entail_sum / (entail_sum + contra_sum + 1e-6)\n",
        "    report.append(f\" NLI ì •í•©ë„: {score:.2f}\")\n",
        "    if not report:\n",
        "        report.append(\" PASS-validation\")\n",
        "    return report\n",
        "\n",
        "# ==============================\n",
        "# 7. í”„ë¡¬í”„íŠ¸ êµ¬ì„± & í…ìŠ¤íŠ¸ ìƒì„±\n",
        "# ==============================\n",
        "def build_prompt(section_obj, project_name, depart_name, project_no, period, budget):\n",
        "    refs = search_reference(section_obj[\"query\"], topk=TOPK)\n",
        "    ctx_block = format_ctx_block(refs)\n",
        "    return f\"\"\"\n",
        "#=========== ìë™ ë¬¸ì¥ ìƒì„±\n",
        "# ì—­í• : {section_obj['role']}\n",
        "# ì‘ì„± í•­ëª©: [{section_obj['section']}]\n",
        "# ì„¸ë¶€ì‚¬ì—…ëª…: {depart_name}\n",
        "# ê³¼ì œë²ˆí˜¸: {project_no}\n",
        "# ê³¼ì œëª…: {project_name}\n",
        "# ê¸°ê°„: {period}\n",
        "# ì˜ˆì‚°: {budget} ì²œì›\n",
        "\n",
        "# ì‘ì„± ì¡°ê±´:\n",
        "# - ë¬¸ì²´: '~í•¨, ~ìŒ, ~ë¨' ë³´ê³ ì„œí˜• ì„œìˆ ì²´\n",
        "# - {', '.join(section_obj['constraints'])} ì¤€ìˆ˜\n",
        "# - ì•„ë˜ ê·¼ê±°ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±\n",
        "{ctx_block}\n",
        "\n",
        "# ìš”ì²­ ë‚´ìš©:\n",
        "# {section_obj['query']}\n",
        "#=========== ì¶œë ¥\n",
        "\"\"\".strip()\n",
        "\n",
        "def generate_text_batch(prompts, batch_size=1):\n",
        "    outputs = []\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generate\"):\n",
        "        chunk = prompts[i:i+batch_size]\n",
        "        out = gen_pipe(chunk, batch_size=batch_size,\n",
        "                       max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
        "                       do_sample=GEN_DO_SAMPLE)\n",
        "        for o in out:\n",
        "            txt = o[0][\"generated_text\"] if isinstance(o, list) else o[\"generated_text\"]\n",
        "            outputs.append(txt)\n",
        "    return outputs\n",
        "\n",
        "# ==============================\n",
        "# 8. DOCX ìƒì„±\n",
        "# ==============================\n",
        "def add_reference_section(doc, all_refs):\n",
        "    doc.add_page_break()\n",
        "    doc.add_heading(\"ê·¼ê±° ë²•ë ¹ ë° ì°¸ê³  ë¬¸ì„œ ëª©ë¡\", level=1)\n",
        "    seen = {}\n",
        "    for r in all_refs:\n",
        "        name = r['source']\n",
        "        seen[name] = seen.get(name, 0) + 1\n",
        "    for name, cnt in seen.items():\n",
        "        doc.add_paragraph(f\"- {name} (ì°¸ì¡° {cnt}íšŒ)\")\n",
        "\n",
        "def render_doc(project_name, depart_name, project_no, period, budget):\n",
        "    doc = Document()\n",
        "    style = doc.styles['Normal']\n",
        "    font = style.font\n",
        "    font.name = 'ë§‘ì€ ê³ ë”•'\n",
        "    font.size = Pt(11)\n",
        "    style._element.rPr.rFonts.set(qn('w:eastAsia'), 'ë§‘ì€ ê³ ë”•')\n",
        "\n",
        "    doc.add_heading(\"ì—°êµ¬ê°œë°œê³„íšì„œ ìë™ìƒì„± ê²°ê³¼\", 0)\n",
        "    doc.add_paragraph(f\"ì„¸ë¶€ì‚¬ì—…ëª…: {depart_name}\")\n",
        "    doc.add_paragraph(f\"ê³¼ì œëª…: {project_name}\")\n",
        "    doc.add_paragraph(f\"ê³¼ì œë²ˆí˜¸: {project_no}\")\n",
        "    doc.add_paragraph(f\"ê¸°ê°„: {period}\")\n",
        "    doc.add_paragraph(f\"ì˜ˆì‚°: {budget} ì²œì›\\n\")\n",
        "\n",
        "    prompts = [build_prompt(s, project_name, depart_name, project_no, period, budget) for s in sections]\n",
        "    generated = generate_text_batch(prompts)\n",
        "\n",
        "    all_refs_flat = []\n",
        "\n",
        "    for sec, gen_text in zip(sections, generated):\n",
        "        doc.add_heading(sec['section'], level=1)\n",
        "        cleaned = clean_generated_text(gen_text)\n",
        "        refs = search_reference(sec[\"query\"], topk=TOPK)\n",
        "        validation = validate_output(sec, cleaned, refs)\n",
        "\n",
        "        doc.add_paragraph(cleaned)\n",
        "\n",
        "       # p = doc.add_paragraph()\n",
        "       # p.add_run(\"[ê·¼ê±°ë¬¸ì„œ(Top-5)]\").bold = True\n",
        "       # for r in refs:\n",
        "       #    doc.add_paragraph(f\"- {r['source']} (w={r['weight']:.2f})\")\n",
        "\n",
        "        p = doc.add_paragraph()\n",
        "        p.add_run(\"[Eval_Result]\").bold = True\n",
        "        for v in validation:\n",
        "            doc.add_paragraph(v)\n",
        "\n",
        "        all_refs_flat.extend(refs)\n",
        "\n",
        "    add_reference_section(doc, all_refs_flat)\n",
        "    outpath = \"RND_Report1027.docx\"\n",
        "    doc.save(outpath)\n",
        "    print(f\"[DONE] â†’ {outpath}\")\n",
        "\n",
        "# ==============================\n",
        "# 9. ì‹¤í–‰ ì˜ˆì‹œ\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    project_name = \"AI ê¸°ë°˜ í‰ë¶€ X-ray ì˜ìƒ íŒë… ìë™í™” ì‹œìŠ¤í…œ ê°œë°œ\"\n",
        "    depart_name  = \"ì‚°ì—…ê¸°ìˆ í˜ì‹ ì‚¬ì—…\"\n",
        "    project_no   = \"RS-2025-00123456\"\n",
        "    period       = \"2025ë…„ 4ì›” 28ì¼ ~\"\n",
        "    budget       = \"5,000,000\"\n",
        "    print(f\"[INFO] ì‹œì‘: {project_name} ({depart_name})\")\n",
        "    render_doc(project_name, depart_name, project_no, period, budget)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python RND_Plan_Pipeline_v2_Stable_fixTokenizer.py"
      ],
      "metadata": {
        "id": "vRqo2odb1dY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import time, os\n",
        "\n",
        "files.download(\"RND_Report.docx\")\n",
        "\n",
        "outpath = \"/content/RND_Report.docx\"\n",
        "if os.path.exists(outpath):\n",
        "    print(\"\\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: RND_Report.docx\")\n",
        "    time.sleep(1)\n",
        "    files.download(outpath)\n",
        "else:\n",
        "    print(\"âš ï¸ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
      ],
      "metadata": {
        "id": "uXWRhGoc5kBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²°ê³¼ ìƒì„± í›„\n",
        "!python /content/RND_Plan_Pipeline_v2_Stable_fixTokenizer.py\n",
        "\n",
        "# ìë™ ë‹¤ìš´ë¡œë“œ\n",
        "from google.colab import files\n",
        "files.download(\"/content/RND_Report.docx\")\n",
        "\n",
        "# ğŸ“„ ë¯¸ë¦¬ë³´ê¸° (í…ìŠ¤íŠ¸)\n",
        "from docx import Document\n",
        "doc = Document(\"/content/RND_Report.docx\")\n",
        "for i, p in enumerate(doc.paragraphs[:10]):\n",
        "    print(f\"[{i+1}] {p.text.strip()}\")"
      ],
      "metadata": {
        "id": "lB-EOyq7-RaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}