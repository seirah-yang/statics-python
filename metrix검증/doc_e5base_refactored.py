# -*- coding: utf-8 -*-
"""
Doc Evaluator (Refactored, Combined Report)
- ì§€ì •ëœ 'í•„ìˆ˜ ì œëª© ë¦¬ìŠ¤íŠ¸'ì— ë§ì¶° ê° ë¬¸ì„œì˜ ì„¹ì…˜ì„ ë§¤ì¹­í•´ í‰ê°€
- ë¬¸ì„œê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°ì—ë„ í†µí•© ë³´ê³ ì„œ 1ê°œë¡œ ì €ì¥
- ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ KPI ì¶”ì¶œ, ê°„ì´ NLI/QA, í¬ë§·Â·ì‘ì§‘ì„± ë“± ì§€í‘œ ì¬ì‚¬ìš©
"""

import os, re, json, math, unicodedata
from typing import List, Dict, Any, Tuple
from collections import namedtuple, Counter, defaultdict

# ------------------------
# ì˜ì¡´ì„±
# ------------------------
try:
    import docx
except Exception as e:
    raise RuntimeError("python-docxê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install python-docx` í›„ ì¬ì‹œì‘í•˜ì„¸ìš”.") from e

_EMBED_OK = False
_BM25_OK = False
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    _EMBED_OK = True
except Exception:
    _EMBED_OK = False

try:
    from rank_bm25 import BM25Okapi
    _BM25_OK = True
except Exception:
    _BM25_OK = False

# ------------------------
# ìœ í‹¸
# ------------------------
def _norm(s: str) -> str:
    # ê³µë°±Â·ë¬¸ì¥ë¶€í˜¸ ì¶•ì•½ + í•œê¸€ ì •ê·œí™”
    s = unicodedata.normalize("NFKC", (s or "").strip())
    s = re.sub(r"\s+", " ", s)
    return s

def split_ko_sentences(text: str) -> List[str]:
    text = re.sub(r"\s+", " ", text or "")
    sents = re.split(r"(?<=[\.?!])\s+|(?<=ë‹¤\.)\s+", text)
    return [s.strip() for s in sents if s and s.strip()]

KPI_PATTERNS = [
    r"\b\d{1,3}\s?%(\s?(ê°ì†Œ|ì¦ê°€|ìœ ì§€))",
    r"(ì˜¤ë¥˜ìœ¨|ì—ëŸ¬ìœ¨)\s*\d{1,3}\s?%",
    r"(ì •í™•ë„|ì™„ì „ì„±|ì¬í˜„ìœ¨|ì •ë°€ë„)\s*(\d{1,3}\s?%)",
    r"(ì‘ë‹µì‹œê°„|ì§€ì—°)\s*\d+(\.\d+)?\s?(ms|ì´ˆ)",
    r"(ì²˜ë¦¬ëŸ‰|TPS|QPS)\s*\d+(\.\d+)?",
    r"(ê¸°ê°„|ë§ˆê°|ë°ë“œë¼ì¸)\s*(\d+\s?(ì¼|ì£¼|ê°œì›”|ì›”|ë¶„ê¸°|ë…„))",
    r"(ë¹„ìš©|ì›ê°€)\s*\d+(,\d{3})*(\.\d+)?\s?(ì›|ë§Œì›|ì–µ)",
]

def extract_numbers_units(text: str) -> List[Tuple[float, str]]:
    out = []
    for m in re.finditer(r"(\d+(?:[\.,]\d+)?)(\s?%|ms|ì´ˆ|ì¼|ì£¼|ê°œì›”|ì›”|ë¶„ê¸°|ë…„|ì›|ë§Œì›|ì–µ)?", text or ""):
        num = m.group(1).replace(",", "")
        unit = (m.group(2) or "").strip()
        try:
            out.append((float(num), unit))
        except Exception:
            pass
    return out

def number_match_quality(a, b) -> Tuple[int, bool]:
    match = 0; conflict = False
    for ax, au in a:
        for bx, bu in b:
            if au and bu and au == bu:
                if bx == 0:
                    continue
                rerr = abs(ax - bx) / (abs(bx) + 1e-6)
                if rerr <= 0.1: match += 1
                elif rerr >= 0.5: conflict = True
    return match, conflict

def keyword_overlap(a: str, b: str) -> float:
    ta = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", (a or "").lower()))
    tb = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", (b or "").lower()))
    if not ta or not tb: return 0.0
    return len(ta & tb) / len(ta | tb)

def ngram_redundancy(sentences: List[str], n: int = 3) -> float:
    grams = []
    for s in sentences:
        toks = re.findall(r"[ê°€-í£A-Za-z0-9]+", (s or "").lower())
        grams += list(zip(*[toks[i:] for i in range(n)]))
    if not grams: return 0.0
    c = Counter(grams)
    dup = sum(v-1 for v in c.values() if v>1)
    return dup / (len(grams) + 1e-6)

def simple_coherence(sentences: List[str]) -> float:
    if len(sentences) < 2: return 0.5
    scores = []
    for i in range(len(sentences)-1):
        scores.append(keyword_overlap(sentences[i], sentences[i+1]))
    return sum(scores)/len(scores)

def format_score(sections: List[Dict[str,str]], required_titles: List[str]) -> float:
    titles = [(s["title"] or "").strip() for s in sections if s.get("title")]
    hit = 0
    for req in required_titles:
        if any(req in (t or "") for t in titles):
            hit += 1
    return hit / max(1, len(required_titles))

# ------------------------
# íŒŒì„œ
# ------------------------
class DocParser:
    def parse(self, docx_path: str):
        try:
            d = docx.Document(docx_path)
        except Exception:
            return None

        paras = [p.text.strip() for p in d.paragraphs if p.text and p.text.strip()]
        sections = []
        cur_title, cur_buf = None, []
        for p in d.paragraphs:
            style = getattr(p.style, "name", "") or ""
            text = (p.text or "").strip()
            if not text:
                continue
            if style.startswith("Heading") or "ì œëª©" in style:
                if cur_title is not None or cur_buf:
                    sections.append({"title": _norm(cur_title), "text": _norm("\n".join(cur_buf))})
                cur_title, cur_buf = text, []
            else:
                cur_buf.append(text)
        if cur_title is not None or cur_buf:
            sections.append({"title": _norm(cur_title), "text": _norm("\n".join(cur_buf))})

        full_text = _norm("\n".join(paras))
        sentences = split_ko_sentences(full_text)
        Doc = namedtuple("Doc", ["sections", "paragraphs", "sentences", "text"])
        return Doc(sections=sections, paragraphs=paras, sentences=sentences, text=full_text)

# ------------------------
# ê²€ìƒ‰ê¸°
# ------------------------
class HybridRetriever:
    def __init__(self, cfg: Dict[str, Any], corpus_texts: List[str]):
        self.cfg = cfg
        self.corpus = corpus_texts or []
        self.use_embed = False
        self.use_bm25  = False

        if _BM25_OK and self.corpus:
            tokenized = [self.tokenize(x) for x in self.corpus]
            self.bm25 = BM25Okapi(tokenized)
            self.use_bm25 = True

        self.embed_dim = None
        if _EMBED_OK and self.corpus and cfg["models"].get("embed"):
            try:
                self.embed = SentenceTransformer(cfg["models"]["embed"])
                self.corpus_vec = self.embed.encode(self.corpus, normalize_embeddings=True)
                self.embed_dim = self.corpus_vec.shape[1]
                self.use_embed = True
            except Exception:
                self.use_embed = False

    @staticmethod
    def tokenize(x: str) -> List[str]:
        return re.findall(r"[ê°€-í£A-Za-z0-9]+", (x or "").lower())

    def topk(self, query: str, k: int = 5) -> List[Tuple[str, float]]:
        cand: Dict[int, float] = {}

        if self.use_bm25:
            scores = self.bm25.get_scores(self.tokenize(query))
            for i, s in enumerate(scores):
                if s > 0: cand[i] = cand.get(i, 0.0) + float(s)

        if self.use_embed:
            qv = self.embed.encode([query], normalize_embeddings=True)[0]
            sims = (self.corpus_vec @ qv)
            for i, s in enumerate(sims):
                cand[i] = cand.get(i, 0.0) + float(s) * 100.0

        if not cand:
            for i, t in enumerate(self.corpus):
                if query[:20] and query[:20] in (t or ""):
                    cand[i] = 1.0

        ranked = sorted(cand.items(), key=lambda x: x[1], reverse=True)[:k]
        return [(self.corpus[i], score) for i, score in ranked]

# ------------------------
# ê²½ëŸ‰ NLI/QA
# ------------------------
class NLIModel:
    def __init__(self, cfg): self.cfg = cfg
    def predict(self, claim: str, evidence: str) -> Dict[str, float]:
        claim_nums = extract_numbers_units(claim)
        evid_nums  = extract_numbers_units(evidence)
        entail = 0.33; contra = 0.33; neutr = 0.34
        match_cnt, conflict = number_match_quality(claim_nums, evid_nums)
        if match_cnt > 0 and not conflict:  entail, contra, neutr = 0.7, 0.1, 0.2
        elif conflict:                       entail, contra, neutr = 0.1, 0.7, 0.2
        elif keyword_overlap(claim, evidence) > 0.4:
                                             entail, contra, neutr = 0.5, 0.1, 0.4
        return {"entail": entail, "contra": contra, "neutral": neutr, "max_p": max(entail, contra, neutr)}

class BooleanQA:
    def __init__(self, cfg): self.cfg = cfg
    def yesno(self, question: str, context: str) -> str:
        ko = keyword_overlap(question.lower(), context)
        cn, en = extract_numbers_units(question), extract_numbers_units(context)
        match_cnt, conflict = number_match_quality(cn, en)
        if conflict: return "no"
        if match_cnt >= 1 or ko > 0.35: return "yes"
        return "no"

# ------------------------
# í‰ê°€ ê³µí†µ
# ------------------------
def extract_claims_from_text(text: str) -> List[str]:
    sents = split_ko_sentences(text or "")
    hits = []
    for s in sents:
        if any(re.search(p, s) for p in KPI_PATTERNS):
            hits.append(s)
    # ìƒí•œì„  & ì¤‘ë³µ ì œê±°
    seen, uniq = set(), []
    for s in hits:
        if s not in seen:
            uniq.append(s); seen.add(s)
    return uniq[:10]

def _fluency(sents: List[str]) -> float:
    if not sents: return 0.5
    lens = [len(s) for s in sents]
    mean_len = sum(lens)/len(lens)
    punct = sum(ch in ".,;:?!~" for s in sents for ch in s) / (sum(lens)+1e-6)
    score = 0.5 + 0.5 * math.tanh((mean_len-25)/50) - 0.2*abs(punct-0.03)
    return max(0.0, min(1.0, score))

# ------------------------
# ì„¹ì…˜ ë§¤ì¹­(ì œëª© ìœ ì‚¬ë„)
# ------------------------
def title_similarity(a: str, b: str) -> float:
    a = _norm(a or ""); b = _norm(b or "")
    if not a or not b: return 0.0
    return keyword_overlap(a, b)  # ê°„ë‹¨íˆ Jaccard ê¸°ë°˜

def map_sections_to_required(sections: List[Dict[str,str]], required_titles: List[str]) -> Dict[str, Dict[str,str]]:
    """
    ê° required_titleì— ëŒ€í•´ ë¬¸ì„œ ë‚´ ê°€ì¥ ìœ ì‚¬í•œ ì„¹ì…˜ì„ 1:1 ë§¤ì¹­.
    ì„ê³„ì¹˜ ë¯¸ë§Œ(ì˜ˆ: 0.25)ì´ë©´ ë¯¸ì¡´ì¬ë¡œ ì²˜ë¦¬.
    """
    mapping = {rt: {"title": None, "text": ""} for rt in required_titles}
    used = set()
    for rt in required_titles:
        best_i, best_sim = -1, 0.0
        for i, sec in enumerate(sections):
            if i in used:
                continue
            sim = title_similarity(rt, sec.get("title", ""))
            if sim > best_sim:
                best_i, best_sim = i, sim
        if best_i >= 0 and best_sim >= 0.25:
            mapping[rt] = {"title": sections[best_i].get("title"), "text": sections[best_i].get("text", "")}
            used.add(best_i)
    return mapping

# ------------------------
# ì„¹ì…˜ í‰ê°€
# ------------------------
def evaluate_section(rt_title: str, sec_text: str, cfg) -> Dict[str, Any]:
    # ê²€ìƒ‰ ì½”í¼ìŠ¤: í•´ë‹¹ ì„¹ì…˜ ë¬¸ì¥
    sents = split_ko_sentences(sec_text)
    retr = HybridRetriever(cfg, sents if sents else [""])
    claims = extract_claims_from_text(sec_text)
    if not claims:
        claims = sents[:5]  # ì£¼ì¥ ì—†ìœ¼ë©´ ëŒ€í‘œë¬¸ì¥ ëŒ€ì²´
    # ê°„ì´ NLI/QA
    nli = NLIModel(cfg["models"].get("nli"))
    qa  = BooleanQA(cfg["models"].get("qna"))

    entail = contra = unknown = 0
    details = []
    for c in claims:
        ev = retr.topk(c, k=3)
        best = ev[0][0] if ev else ""
        nout = nli.predict(c, best)
        conf = nout["max_p"]
        if conf >= 0.65:
            verdict = max((("entailment", nout["entail"]), ("contradiction", nout["contra"]), ("neutral", nout["neutral"])), key=lambda x:x[1])[0]
        else:
            verdict = "entailment" if qa.yesno(f"Is the claim supported? {c}", best).startswith("y") else "contradiction"
            conf = max(0.65, conf)
        if verdict == "entailment": entail += 1
        elif verdict == "contradiction": contra += 1
        else: unknown += 1
        details.append({"claim": c, "evidence": (best or "")[:200], "verdict": verdict, "confidence": conf})

    tot = max(1, entail+contra+unknown)
    accuracy = entail / tot
    flu = _fluency(sents)
    coh = simple_coherence(sents)
    red = ngram_redundancy(sents, n=3)

    return {
        "required_title": rt_title,
        "exists": bool(sec_text.strip()),
        "accuracy": float(accuracy),
        "fluency": float(flu),
        "coherence": float(coh),
        "redundancy": float(red),
        "kpi_count": sum(1 for s in sents if any(re.search(p, s) for p in KPI_PATTERNS)),
        "length_chars": len(sec_text),
        "details": details
    }

# ------------------------
# í†µí•© ì‹¤í–‰ê¸°
# ------------------------
def run_combined_report(docx_paths: List[str], required_titles: List[str], cfg: Dict[str, Any], out_path: str):
    parser = DocParser()
    all_results = defaultdict(list)  # key: required_title -> [ {doc, metrics...}, ... ]
    doc_level = []                   # ë¬¸ì„œë³„ ì „ì²´ í¬ë§·/ì‘ì§‘ì„± ë“±

    for path in docx_paths:
        doc = parser.parse(path)
        if not doc:
            doc_level.append({"doc": path, "parse_ok": False})
            continue

        # ë¬¸ì„œ ìˆ˜ì¤€ í¬ë§· ì ìˆ˜(í•„ìˆ˜ ì œëª© ì»¤ë²„ë¦¬ì§€)
        fmt = format_score(doc.sections, required_titles)
        coh = simple_coherence(doc.sentences)
        flu = _fluency(doc.sentences)
        red = ngram_redundancy(doc.sentences, n=3)
        doc_level.append({
            "doc": path, "parse_ok": True,
            "format": float(fmt), "coherence": float(coh), "fluency": float(flu), "redundancy": float(red)
        })

        # ì œëª© ë§¤í•‘ â†’ ì„¹ì…˜ë³„ í‰ê°€
        mapped = map_sections_to_required(doc.sections, required_titles)
        for rt in required_titles:
            sec_text = mapped[rt].get("text", "")
            sec_res = evaluate_section(rt, sec_text, cfg)
            sec_res["doc"] = path
            sec_res["matched_title"] = mapped[rt].get("title")
            all_results[rt].append(sec_res)

    # í†µí•© ìŠ¤ì½”ì–´ ì§‘ê³„(ì œëª©ë³„ í‰ê· )
    title_summary = {}
    for rt, items in all_results.items():
        if not items:
            continue
        def avg(k):
            vals = [x[k] for x in items if isinstance(x.get(k), (int, float))]
            return sum(vals)/len(vals) if vals else 0.0
        title_summary[rt] = {
            "coverage": sum(1 for x in items if x["exists"]) / max(1, len(items)),
            "accuracy": avg("accuracy"),
            "fluency": avg("fluency"),
            "coherence": avg("coherence"),
            "redundancy": avg("redundancy"),
            "avg_kpi": avg("kpi_count"),
            "avg_len": avg("length_chars"),
        }

    # ìµœì¢… ë³´ê³ ì„œ ì‘ì„±
    lines = []
    lines.append("# í†µí•© ë¬¸ì„œ í‰ê°€ ë³´ê³ ì„œ\n\n")
    lines.append(f"- ì´ ë¬¸ì„œ ìˆ˜: {len(docx_paths)}\n")
    lines.append(f"- í•„ìˆ˜ ì œëª© ìˆ˜: {len(required_titles)}\n\n")

    lines.append("## 1) ë¬¸ì„œ ìˆ˜ì¤€ ê°œìš”\n")
    for row in doc_level:
        if not row.get("parse_ok"):
            lines.append(f"- {row['doc']}: íŒŒì‹± ì‹¤íŒ¨\n")
            continue
        lines.append(f"- {row['doc']}: Format={row['format']:.2f}, Coherence={row['coherence']:.2f}, Fluency={row['fluency']:.2f}, Redundancy={row['redundancy']:.2f}\n")
    lines.append("\n")

    lines.append("## 2) ì œëª©(ì„¹ì…˜)ë³„ ìš”ì•½(í‰ê· )\n")
    lines.append("| ì œëª© | ì»¤ë²„ë¦¬ì§€ | Accuracy | Fluency | Coherence | Redundancy(â†“) | KPIê°œìˆ˜ | ê¸¸ì´ |\n")
    lines.append("|---|---:|---:|---:|---:|---:|---:|---:|\n")
    for rt in required_titles:
        ts = title_summary.get(rt, None)
        if not ts:
            lines.append(f"| {rt} | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0 |\n")
        else:
            lines.append(f"| {rt} | {ts['coverage']:.2f} | {ts['accuracy']:.2f} | {ts['fluency']:.2f} | {ts['coherence']:.2f} | {ts['redundancy']:.2f} | {ts['avg_kpi']:.2f} | {int(ts['avg_len'])} |\n")
    lines.append("\n")

    lines.append("## 3) ìƒì„¸(ë¬¸ì„œÃ—ì œëª©)\n")
    for rt in required_titles:
        lines.append(f"### [{rt}]\n")
        items = all_results.get(rt, [])
        if not items:
            lines.append("- í•´ë‹¹ ì„¹ì…˜ ì—†ìŒ\n\n");
            continue
        for it in items:
            lines.append(f"- ë¬¸ì„œ: {it['doc']}\n")
            lines.append(f"  - ë§¤ì¹­ëœ ì œëª©: {it.get('matched_title')}\n")
            lines.append(f"  - ì¡´ì¬ì—¬ë¶€: {it['exists']} | Accuracy={it['accuracy']:.2f}, Fluency={it['fluency']:.2f}, Coherence={it['coherence']:.2f}, Redundancy={it['redundancy']:.2f}, KPI={it['kpi_count']}, ê¸¸ì´={it['length_chars']}\n")
            # ì„¸ë¶€ í´ë ˆì„ ìƒìœ„ 3ê°œë§Œ
            for d in it["details"][:3]:
                ev = d["evidence"].replace("\n", " ")
                lines.append(f"    - ì£¼ì¥: {d['claim']}\n")
                lines.append(f"      Â· íŒì •: {d['verdict']} (ì‹ ë¢°ë„ {d['confidence']:.2f})\n")
                lines.append(f"      Â· ê·¼ê±°: {ev[:200]}...\n")
        lines.append("\n")

    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("".join(lines))
    return out_path

# ------------------------
# ê¸°ë³¸ ì„¤ì • & ì‹¤í–‰ ì˜ˆì‹œ
# ------------------------
REQUIRED_TITLES = [
    "ì—°êµ¬ê°œë°œ ëª©í‘œ",
    "ì—°êµ¬ê°œë°œ ë‚´ìš©",
    "ì—°êµ¬ê°œë°œì„±ê³¼ í™œìš©ê³„íš ë° ê¸°ëŒ€íš¨ê³¼",
    "ì—°êµ¬ê¸°íšê³¼ì œì˜ ê°œìš”",
    "ì—°êµ¬ê°œë°œê³¼ì œì˜ ë°°ê²½",
    "ì—°êµ¬ê°œë°œê³¼ì œì˜ í•„ìš”ì„±",
    "ë³´ì•ˆë“±ê¸‰ì˜ ë¶„ë¥˜ ë° í•´ë‹¹ ì‚¬ìœ ",
    "ê¸°ìˆ ê°œë°œ í•µì‹¬ì–´(í‚¤ì›Œë“œ)",
    "ì—°ì°¨ë³„ ê°œë°œëª©í‘œ",
    "ì—°ì°¨ë³„ ê°œë°œë‚´ìš© ë° ë²”ìœ„",
    "ì¶”ì§„ë°©ë²• ë° ì „ëµ",
    "ê³¼ì œ ì„±ê³¼ì˜ í™œìš©ë°©ì•ˆ",
    "ì‹ ê·œì‚¬ì—… ì‹ ì„¤ì˜ ê¸°ëŒ€íš¨ê³¼",
    "ì‚¬íšŒì  ê°€ì¹˜ ì°½ì¶œ ê³„íš",
    "ì‚¬íšŒì  ê°€ì¹˜ì°½ì¶œì˜ ê¸°ëŒ€íš¨ê³¼",
    "ê²½ì œì  ì„±ê³¼ì°½ì¶œì˜ ê¸°ëŒ€íš¨ê³¼",
    "ì‹ ê·œ ì¸ë ¥ ì±„ìš© ê³„íš ë° í™œìš© ë°©ì•ˆ",
]

DOCX_LIST = ['/content/e5base_ê³„íšì„œ930.docx']
# ì„¤ì • ê°ì²´ (ì„ë² ë”© ëª¨ë¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤. snunlp/KR-SBERT-V40KëŠ” í”íˆ ì‚¬ìš©ë˜ëŠ” í•œêµ­ì–´ SBERT ëª¨ë¸ ì´ë¦„ ì˜ˆì‹œ)
CONFIG = {"models": {"embed": "snunlp/KR-SBERT-V40K", "nli": {}, "qna": {}}}

if not DOCX_LIST:
    print("[ì•Œë¦¼] DOCX_LISTì— ë¬¸ì„œ ê²½ë¡œë¥¼ 17ê°œ ë„£ì–´ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    # ğŸš¨ run_combined_report í˜¸ì¶œ ì¶”ê°€
    out_path = "/content/result/e5l0930_REPORT.txt"
    run_combined_report(DOCX_LIST, REQUIRED_TITLES, CONFIG, out_path) 
    print(" ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ:", out_path)
