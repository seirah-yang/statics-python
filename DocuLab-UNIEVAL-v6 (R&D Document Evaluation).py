!pip install html2text
!pip install sentence-transformers transformers torch python-docx PyPDF2 scikit-learn pandas tqdm

import torch
print("CUDA available:", torch.cuda.is_available())
# ===========================================================
# Folder-based Evaluation + Visualization Report (Colab)
# ===========================================================
import os, re, sys, subprocess, time
import pandas as pd, numpy as np
from pathlib import Path
from tqdm import tqdm

# --- Optional: lightweight install guards (Colab-friendly)
def _maybe_pip_install(pkg):
    try:
        __import__(pkg)
    except Exception:
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", pkg], check=False)

for _pkg in ["python-docx", "PyPDF2", "sentence_transformers", "transformers", "plotly", "html2text"]:
    # import name != pip name for python-docx/sentence-transformers handled above
    pass

# Core imports (after optional install)
from docx import Document
from PyPDF2 import PdfReader
import plotly.graph_objects as go

import torch
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline

# -------------------------------
# 1ï¸âƒ£ Google Drive mount
# -------------------------------
try:
    from google.colab import drive, files
    drive.mount('/content/drive')
    IN_COLAB = True
except Exception:
    IN_COLAB = False
    print("[WARN] Not running in Colab. Drive mount skipped.")
SAVE_DIR = "/content/drive/MyDrive/unieval_results" if IN_COLAB else "./unieval_results"
os.makedirs(SAVE_DIR, exist_ok=True)

# -------------------------------
# 2ï¸âƒ£ Core model setup
# -------------------------------
EMBEDDING_MODEL = "intfloat/e5-large"
NLI_MODEL = "joeddav/xlm-roberta-large-xnli"
WEIGHTS = {"accuracy":0.35,"relevance":0.15,"coherence":0.15,
           "fluency":0.10,"consistency":0.15,"redundancy":0.10}
REDUNDANCY_TH = 0.90

_embedder, _nli = None, None

def get_embedder():
    global _embedder
    if _embedder is None:
        _embedder = SentenceTransformer(EMBEDDING_MODEL)
    return _embedder

def get_nli():
    """
    Robust NLI pipeline:
    - return_all_scores=True â†’ ì˜ˆì¸¡ ë¶„í¬ í™•ë³´
    - truncation=True â†’ ê¸´ ë¬¸ì¥ ì•ˆì „ ì²˜ë¦¬
    - device ìë™ ì„ íƒ
    """
    global _nli
    if _nli is None:
        tok = AutoTokenizer.from_pretrained(NLI_MODEL)
        mdl = AutoModelForSequenceClassification.from_pretrained(NLI_MODEL)
        device = 0 if torch.cuda.is_available() else -1
        _nli = TextClassificationPipeline(
            model=mdl,
            tokenizer=tok,
            return_all_scores=True,
            truncation=True,
            device=device
        )
    return _nli

# -------------------------------
# 3ï¸âƒ£ Text loading utilities
# -------------------------------
def load_text(path):
    suf = Path(path).suffix.lower()
    try:
        if suf==".docx":
            return "\n".join([p.text for p in Document(path).paragraphs])
        elif suf==".pdf":
            return "\n".join([(p.extract_text() or "") for p in PdfReader(path).pages])
        elif suf==".txt":
            return Path(path).read_text(encoding="utf-8", errors="ignore")
        else:
            print(f"[INFO] Skip unsupported file: {path}")
            return ""
    except Exception as e:
        print(f"[ERROR] load_text failed for {path}: {e}")
        return ""

def sent_split(text):
    # í•œêµ­ì–´ ì¢…ê²°(ë‹¤/ìš”/ê¹Œ/í•¨) + ì˜ë¬¸ ë¬¸ì¥ë¶€í˜¸ í˜¼ìš©
    # ë„ˆë¬´ ì§§ì€ í† í°(2ì ì´í•˜) ì œê±°
    chunks = re.split(r'(?<=[\.!\?]|ë‹¤|ìš”|ê¹Œ|í•¨)\s+', text)
    return [s.strip() for s in chunks if len(s.strip()) > 2]

# -------------------------------
# 4ï¸âƒ£ Metric functions
# -------------------------------
def cosine_redundancy(sents, emb):
    """1(ì¢‹ìŒ) ~ 0(ë‚˜ì¨): ë¬¸ì¥ ê°„ ê³¼ë„í•œ ì¤‘ë³µì´ ë§ì„ìˆ˜ë¡ ì ìˆ˜â†“"""
    n = len(sents)
    if n < 2:
        return 1.0
    embs = emb.encode(sents, normalize_embeddings=True)
    sim = util.cos_sim(embs, embs).cpu().numpy()
    # ìƒì‚¼ê°ë§Œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì¹´ìš´íŠ¸ (ëŒ€ì¹­/ìê¸°ìì‹  ì œì™¸)
    triu_idx = np.triu_indices(n, k=1)
    pair_sims = sim[triu_idx]
    redundant_ratio = np.mean(pair_sims > REDUNDANCY_TH) if pair_sims.size > 0 else 0.0
    return float(1 - redundant_ratio)

def coherence_score(sents, emb):
    """ì¸ì ‘ ë¬¸ì¥ê°„ ì˜ë¯¸ ìœ ì‚¬ë„ í‰ê·  (ë†’ì„ìˆ˜ë¡ ì‘ì§‘/coherenceâ†‘)"""
    if len(sents) < 2:
        return 0.0
    embs = emb.encode(sents, normalize_embeddings=True)
    sims = [util.cos_sim(embs[i], embs[i+1]).item() for i in range(len(embs)-1)]
    return float(np.mean(sims)) if sims else 0.0

def fluency_score(sents):
    """ë¬¸ì¥ ê¸¸ì´ ë¶„ì‚° ê¸°ë°˜ ìœ ì°½ì„± ê·¼ì‚¬ì¹˜ (ê¸¸ì´ í‘œì¤€í¸ì°¨/í‰ê·  ë¹„ìœ¨ ì—­ìˆ˜)"""
    tokens = [len(s.split()) for s in sents]
    if not tokens:
        return 0.0
    mean_len = np.mean(tokens)
    std_len = np.std(tokens)
    ratio = max(0.0, 1 - std_len / (mean_len + 1e-6))
    return float(round(ratio, 3))

def relevance_score(title, text, emb):
    """ì œëª©(query) vs ë³¸ë¬¸(text)ì˜ ì˜ë¯¸ ìœ ì‚¬ë„"""
    if not text.strip():
        return 0.0
    embs = emb.encode([f"query: {title}", text], normalize_embeddings=True)
    return float(util.cos_sim(embs[0], embs[1]))

def _parse_nli_result(res):
    """HF pipeline ê²°ê³¼ë¥¼ ì•ˆì „ íŒŒì‹±í•˜ì—¬ (label, score) ì¤‘ ìŠ¤ì½”ì–´ ìµœëŒ€ í•­ëª© ë°˜í™˜"""
    # ê°€ëŠ¥í•œ í˜•íƒœ: dict / [dict] / [[dict,...]]
    first = res
    if isinstance(first, list) and first:
        first = first[0]
    if isinstance(first, list) and first:
        best = max(first, key=lambda x: x.get("score", 0.0))
    elif isinstance(first, dict):
        best = first
    else:
        # ì˜ˆìƒì¹˜ ëª»í•œ êµ¬ì¡° â†’ ë³´ìˆ˜ì  ì²˜ë¦¬
        return ("NEUTRAL", 0.0)
    return (best.get("label", "NEUTRAL").upper(), best.get("score", 0.0))

def consistency_score(sents):
    """
    ë¬¸ì¥ ê°„ ë…¼ë¦¬ ì¼ê´€ì„±: ì¸ì ‘ ë¬¸ì¥ ê°„ NLIë¡œ CONTRADICTION ë¹„ìœ¨ì„ íŒ¨ë„í‹°í™”
    ì ìˆ˜ = 1 - contra / max(entail+contra, 1)
    """
    if len(sents) < 2:
        return 1.0
    nli = get_nli()
    pairs = [(sents[i], sents[i+1]) for i in range(len(sents)-1)]
    entail = contra = unknown = 0

    for a, b in pairs:
        try:
            res = nli({"text": a, "text_pair": b})
            lab, _ = _parse_nli_result(res)
            if "ENTAIL" in lab:
                entail += 1
            elif "CONTRAD" in lab:
                contra += 1
            else:
                unknown += 1
        except Exception as e:
            # íŒŒì´í”„ë¼ì¸/í† í°í™” ì˜¤ë¥˜ ë“±ì€ unknown ì²˜ë¦¬
            unknown += 1

    denom = max(entail + contra, 1)
    score = 1.0 - (contra / denom)
    return float(max(0.0, min(1.0, score)))

def accuracy_score(claims, evid):
    """
    Accuracy(ì •í•©ì„±): claim ë¬¸ì¥(ìˆ«ì/ëª©í‘œ í¬í•¨) vs evidence(ê¸°íƒ€ ë¬¸ì¥) ê°„
    ìµœëŒ€ ìœ ì‚¬ë„ì˜ í‰ê· . 0~1 (ë†’ì„ìˆ˜ë¡ claimì´ ë‚´ë¶€ ê·¼ê±°ë¡œ ì˜ ë’·ë°›ì¹¨ë¨)
    """
    if not claims or not evid:
        return 0.0
    emb = get_embedder()
    c_emb = emb.encode(claims, normalize_embeddings=True)
    e_emb = emb.encode(evid, normalize_embeddings=True)
    sims = util.cos_sim(c_emb, e_emb).cpu().numpy()
    return float(np.mean(np.max(sims, axis=1))) if sims.size > 0 else 0.0

# -------------------------------
# 5ï¸âƒ£ Evaluation functions
# -------------------------------
def evaluate_section(title, text):
    emb = get_embedder()
    sents = sent_split(text)

    # ê°„ë‹¨í•œ claim ì¶”ì¶œ íœ´ë¦¬ìŠ¤í‹±(ìˆ«ì/ëª©í‘œ/ì„±ê³¼/ë‹¬ì„± í‚¤ì›Œë“œ)
    claims = [s for s in sents if re.search(r'\d|ëª©í‘œ|ì„±ê³¼|ë‹¬ì„±', s)]
    evid   = [s for s in sents if s not in claims]

    metrics = {
        "accuracy":    accuracy_score(claims, evid),
        "relevance":   relevance_score(title, text, emb),
        "coherence":   coherence_score(sents, emb),
        "fluency":     fluency_score(sents),
        "consistency": consistency_score(sents),
        "redundancy":  cosine_redundancy(sents, emb),
    }
    metrics["final"] = float(sum(metrics[k] * WEIGHTS[k] for k in WEIGHTS))
    return metrics

def evaluate_folder(target_folder):
    target = Path(target_folder)
    files_to_eval = [f for f in target.glob("*") if f.suffix.lower() in [".docx",".pdf",".txt"]]

    if not files_to_eval:
        print(f"[WARN] No evaluable files in: {target_folder}")
        return pd.DataFrame([])

    all_results = []
    for f in tqdm(files_to_eval, desc="Evaluate"):
        try:
            text = load_text(f)
            title = f.stem
            result = evaluate_section(title, text)
            result["file"] = f.name
            all_results.append(result)
            print(f"âœ… {f.name} â†’ í‰ê°€ì™„ë£Œ (final={result['final']:.3f})")
        except Exception as e:
            print(f"âŒ {f.name} â†’ í‰ê°€ ì‹¤íŒ¨: {e}")

    if not all_results:
        print("[WARN] No results produced.")
        return pd.DataFrame([])

    df = pd.DataFrame(all_results)
    csv_path = os.path.join(SAVE_DIR, "results_summary.csv")
    df.to_csv(csv_path, index=False)
    print(f"\nğŸ“Š CSV ì €ì¥ ì™„ë£Œ: {csv_path}")

    generate_report_html(df, SAVE_DIR)
    return df

# -------------------------------
# 6ï¸âƒ£ Visualization Report
# -------------------------------
def generate_report_html(df, save_dir):
    if df is None or df.empty:
        print("[WARN] Empty DataFrame. Skip report generation.")
        return

    metrics = ["accuracy","relevance","coherence","fluency","consistency","redundancy"]
    # ê²°ì¸¡ ë°©ì§€
    for m in metrics + ["final"]:
        if m not in df.columns:
            df[m] = np.nan
    df = df.fillna(0.0)

    mean_scores = df[metrics].mean(numeric_only=True)
    values = [float(mean_scores.get(m, 0.0)) for m in metrics]

    # Radar Chart
    radar = go.Figure()
    radar.add_trace(go.Scatterpolar(
        r=values + [values[0]],
        theta=metrics + [metrics[0]],
        fill='toself',
        name='í‰ê·  ì ìˆ˜'
    ))
    radar.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0,1])),
        title="DocuLab UNIEVAL R&D Radar Chart"
    )

    # Bar Chart
    bar = go.Figure([go.Bar(x=df["file"], y=df["final"])])
    bar.update_layout(title="ë¬¸ì„œë³„ Final Score", xaxis_title="ë¬¸ì„œ", yaxis_title="ì ìˆ˜(0~1)")

    # Combine HTML
    html_path = os.path.join(save_dir, "evaluation_report.html")
    with open(html_path, "w", encoding="utf-8") as f:
        f.write("<h2>DocuLab UNIEVAL R&D Evaluation Report</h2>")
        f.write("<p>ìë™ ë¬¸ì„œ í’ˆì§ˆí‰ê°€ ê²°ê³¼ì…ë‹ˆë‹¤.</p>")
        f.write(radar.to_html(full_html=False, include_plotlyjs='cdn'))
        f.write(bar.to_html(full_html=False, include_plotlyjs=False))
        f.write(df.to_html(index=False))
    print(f"ğŸ“ˆ ì‹œê°í™” ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {html_path}")

# -------------------------------
# 7ï¸âƒ£ Example Run
# -------------------------------
target_folder = "/content/drive/MyDrive/1027" if IN_COLAB else "./samples"
df = evaluate_folder(target_folder)   # â†’ results_summary.csv + evaluation_report.html ìƒì„±

# ===========================================================
# 8ï¸âƒ£ HTML â†’ Markdown ë³€í™˜
# ===========================================================
html_path = os.path.join(SAVE_DIR, "evaluation_report.html")
outpath = "/content/evaluation_report.md" if IN_COLAB else "./evaluation_report.md"

if os.path.exists(html_path):
    try:
        import html2text
    except Exception:
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", "html2text"], check=False)
        import html2text

    with open(html_path, "r", encoding="utf-8") as f:
        html_content = f.read()
    md_content = html2text.html2text(html_content)

    with open(outpath, "w", encoding="utf-8") as f:
        f.write(md_content)

    print("\nâœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ:", outpath)
    if IN_COLAB:
        from google.colab import files
        time.sleep(1)
        files.download(outpath)
else:
    print("âš ï¸ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
