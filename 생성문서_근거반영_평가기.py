# -*- coding: utf-8 -*-
import os
import docx
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1ï¸âƒ£ ì„ë² ë” ìºì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_EMBEDDER_CACHE = {"name": None, "model": None}

def _get_embedder(model_name="intfloat/e5-large"):
    global _EMBEDDER_CACHE
    if _EMBEDDER_CACHE["model"] and _EMBEDDER_CACHE["name"] == model_name:
        return _EMBEDDER_CACHE["model"]
    model = SentenceTransformer(model_name)
    _EMBEDDER_CACHE["name"] = model_name
    _EMBEDDER_CACHE["model"] = model
    return model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2ï¸âƒ£ íŒŒì¼ ë¡œë“œ í•¨ìˆ˜ (DOCX/PDF ìë™ íŒë³„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_text_from_file(path):
    ext = os.path.splitext(path)[1].lower()
    if ext == ".docx":
        doc = docx.Document(path)
        return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
    elif ext == ".pdf":
        reader = PdfReader(path)
        return "\n".join(page.extract_text() for page in reader.pages if page.extract_text())
    else:
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3ï¸âƒ£ í´ë” ë‚´ ëª¨ë“  ê·¼ê±°ë¬¸í—Œ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_law_corpus_from_dir(dir_path):
    corpus = []
    for file in os.listdir(dir_path):
        path = os.path.join(dir_path, file)
        if not os.path.isfile(path):
            continue
        if path.endswith((".docx", ".pdf")):
            try:
                text = load_text_from_file(path)
                if text.strip():
                    corpus.append(text)
            except Exception as e:
                print(f"[WARN] {file} ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    return corpus

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4ï¸âƒ£ ê·¼ê±°ë¬¸í—Œ ì¤€ìˆ˜ë„ í‰ê°€ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def reference_compliance_with_sources(
    section_text,
    law_corpus,
    model_name="intfloat/e5-large",
    threshold=0.8,
    top_k=5
):
    if not section_text or not law_corpus:
        return {"compliance_score": 0.0, "top_references": []}

    model = _get_embedder(model_name)

    emb_section = model.encode([section_text], normalize_embeddings=True)
    emb_law = model.encode(law_corpus, normalize_embeddings=True)

    sims = cosine_similarity(emb_section, emb_law)[0]
    matched_ratio = float((sims >= threshold).sum()) / len(law_corpus)
    top_idx = sims.argsort()[::-1][:top_k]
    top_refs = [(law_corpus[i][:200], float(sims[i])) for i in top_idx]

    return {
        "compliance_score": round(float(matched_ratio), 3),
        "top_references": top_refs
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5ï¸âƒ£ ì—¬ëŸ¬ íŒŒì¼ ì¼ê´„ í‰ê°€ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # (1) í‰ê°€ ëŒ€ìƒ ë¬¸ì„œ í´ë” ê²½ë¡œ
    target_dir = "/content/drive/MyDrive/1027"  # â† í‰ê°€í•  ë¬¸ì„œ í´ë” ì…ë ¥ (.docx / .pdf)
    law_dir = "/content/drive/MyDrive/reference_file"        # â† ê·¼ê±°ë¬¸í—Œ í´ë” ì…ë ¥

    # (2) ê·¼ê±°ë¬¸í—Œ ë¡œë“œ
    law_corpus = load_law_corpus_from_dir(law_dir)

    # (3) í‰ê°€ ëŒ€ìƒ í´ë” ìˆœíšŒ
    results = []
    for file in os.listdir(target_dir):
        file_path = os.path.join(target_dir, file)
        if not os.path.isfile(file_path):
            continue
        if file_path.endswith((".docx", ".pdf")):
            print(f"\nğŸ“„ í‰ê°€ ì¤‘: {file}")
            section_text = load_text_from_file(file_path)
            result = reference_compliance_with_sources(section_text, law_corpus)
            results.append({
                "file": file,
                "compliance_score": result["compliance_score"]
            })
            print(f" â†’ ì¤€ìˆ˜ë„ ì ìˆ˜: {result['compliance_score']:.3f}")

    # (4) ì „ì²´ ìš”ì•½ ì¶œë ¥
    print("\nâœ… í‰ê°€ ì™„ë£Œ ê²°ê³¼ ìš”ì•½:")
    for r in results:
        print(f"{r['file']:50s} | Score: {r['compliance_score']:.3f}")
