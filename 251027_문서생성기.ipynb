{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg90EsI+hamzNB1T4Kiao8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seirah-yang/F_roject/blob/main/251027_%EB%AC%B8%EC%84%9C%EC%83%9D%EC%84%B1%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJXDakYlEObS"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os, re, json, math, unicodedata\n",
        "from typing import List, Dict, Any, Optional\n",
        "from collections import namedtuple, Counter, defaultdict\n",
        "\n",
        "# ── Optional deps & graceful fallbacks ────────────────────────────────────────\n",
        "_HAS_SENTENCE_TRANSFORMERS = False\n",
        "_HAS_SKLEARN = False\n",
        "_HAS_DOCX = False\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer, util  # type: ignore\n",
        "    _HAS_SENTENCE_TRANSFORMERS = True\n",
        "except Exception:\n",
        "    SentenceTransformer = Any  # type: ignore\n",
        "    util = None  # type: ignore\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity  # type: ignore\n",
        "    _HAS_SKLEARN = True\n",
        "except Exception:\n",
        "    np = None  # type: ignore\n",
        "    cosine_similarity = None  # type: ignore\n",
        "\n",
        "try:\n",
        "    import docx  # python-docx\n",
        "    _HAS_DOCX = True\n",
        "except Exception:\n",
        "    docx = None  # type: ignore\n",
        "\n",
        "# ── Text utils ────────────────────────────────────────────────────────────────\n",
        "def _norm(s: Optional[str]) -> str:\n",
        "    s = unicodedata.normalize(\"NFKC\", (s or \"\").strip())\n",
        "    # 제목 앞번호 제거: 'Ⅰ.', '1.', '2-' 등\n",
        "    s = re.sub(r\"^[ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩ0-9\\.\\-]+\\s*\", \"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def split_ko_sentences(text: Optional[str]) -> List[str]:\n",
        "    \"\"\"한국어 문장 분리 (간단 버전)\"\"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text or \"\")\n",
        "    # 마침표/물음표/느낌표 + '다.' 패턴 기준 간단 분리\n",
        "    sents = re.split(r\"(?<=[\\.?!])\\s+|(?<=다\\.)\\s+\", text)\n",
        "    return [s.strip() for s in sents if s.strip()]\n",
        "\n",
        "_TOKEN_RE = re.compile(r\"[A-Za-z가-힣0-9%\\.]+\", re.UNICODE)\n",
        "\n",
        "def _simple_tokenize(text: Optional[str]) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    return [t for t in _TOKEN_RE.findall(text.lower()) if t]\n",
        "\n",
        "def keyword_overlap(a: Optional[str], b: Optional[str]) -> float:\n",
        "    ta = set(re.findall(r\"[가-힣A-Za-z0-9]+\", (a or \"\").lower()))\n",
        "    tb = set(re.findall(r\"[가-힣A-Za-z0-9]+\", (b or \"\").lower()))\n",
        "    if not ta or not tb:\n",
        "        return 0.0\n",
        "    return len(ta & tb) / len(ta | tb)\n",
        "\n",
        "def ngram_redundancy(sentences: List[str], n: int = 3) -> float:\n",
        "    grams = []\n",
        "    for s in sentences:\n",
        "        toks = re.findall(r\"[가-힣A-Za-z0-9]+\", (s or \"\").lower())\n",
        "        if len(toks) >= n:\n",
        "            grams += list(zip(*[toks[i:] for i in range(n)]))\n",
        "    if not grams:\n",
        "        return 0.0\n",
        "    c = Counter(grams)\n",
        "    dup = sum(v - 1 for v in c.values() if v > 1)\n",
        "    return dup / (len(grams) + 1e-6)\n",
        "\n",
        "# ── Embedding cache ───────────────────────────────────────────────────────────\n",
        "_EMBEDDER_CACHE: Dict[str, Any] = {\"name\": None, \"model\": None}\n",
        "\n",
        "def _get_embedder(model_name: str = \"intfloat/e5-large\"):\n",
        "    if not _HAS_SENTENCE_TRANSFORMERS:\n",
        "        raise RuntimeError(\"sentence_transformers 미설치\")\n",
        "    global _EMBEDDER_CACHE\n",
        "    if _EMBEDDER_CACHE[\"model\"] is not None and _EMBEDDER_CACHE[\"name\"] == model_name:\n",
        "        return _EMBEDDER_CACHE[\"model\"]\n",
        "    model = SentenceTransformer(model_name)\n",
        "    _EMBEDDER_CACHE[\"name\"] = model_name\n",
        "    _EMBEDDER_CACHE[\"model\"] = model\n",
        "    return model\n",
        "\n",
        "def cosine_redundancy(\n",
        "    sentences: List[str],\n",
        "    model_name: str = \"intfloat/e5-large\",\n",
        "    threshold: float = 0.9\n",
        ") -> float:\n",
        "    \"\"\"코사인 유사도 기반 반복률 계산 (임베딩 실패 시 0.0 폴백)\"\"\"\n",
        "    sentences = [s for s in sentences if s and s.strip()]\n",
        "    if len(sentences) < 2:\n",
        "        return 0.0\n",
        "    if not (_HAS_SENTENCE_TRANSFORMERS and _HAS_SKLEARN and np is not None):\n",
        "        # 임베딩/스케일러 불가 → 안전 폴백\n",
        "        return 0.0\n",
        "    try:\n",
        "        model = _get_embedder(model_name)\n",
        "        embeddings = model.encode(sentences, normalize_embeddings=True)\n",
        "        sims = cosine_similarity(embeddings)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "    n = len(sentences)\n",
        "    total_pairs, high_pairs = 0, 0\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            total_pairs += 1\n",
        "            if sims[i, j] >= threshold:\n",
        "                high_pairs += 1\n",
        "    return high_pairs / total_pairs if total_pairs else 0.0\n",
        "\n",
        "def simple_coherence(sentences: List[str]) -> float:\n",
        "    if len(sentences) < 2:\n",
        "        return 0.5\n",
        "    scores = [keyword_overlap(sentences[i], sentences[i + 1]) for i in range(len(sentences) - 1)]\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "def _fluency(sents: List[str]) -> float:\n",
        "    if not sents:\n",
        "        return 0.5\n",
        "    lens = [len(s) for s in sents]\n",
        "    mean_len = sum(lens) / len(lens)\n",
        "    punct = sum(ch in \".,;:?!~\" for s in sents for ch in s) / (sum(lens) + 1e-6)\n",
        "    score = 0.5 + 0.5 * math.tanh((mean_len - 25) / 50) - 0.2 * abs(punct - 0.03)\n",
        "    return max(0.0, min(1.0, score))\n",
        "\n",
        "def relevance_score(section_text: str, required_title: str) -> float:\n",
        "    return keyword_overlap(section_text, required_title)\n",
        "\n",
        "def consistency_score(section_text: str) -> float:\n",
        "    # 숫자 + 단위 패턴의 다양성으로 단순 일관성 근사\n",
        "    nums = re.findall(r\"\\d+(?:[\\.,]\\d+)?\\s?(%|ms|초|일|주|개월|월|분기|년|원|만원|억)?\", section_text or \"\")\n",
        "    units = [u.strip() for u in nums if u and isinstance(u, str) and u.strip()]\n",
        "    return len(set(units)) / len(units) if units else 1.0\n",
        "\n",
        "# ── Docx parser ───────────────────────────────────────────────────────────────\n",
        "class DocParser:\n",
        "    def parse(self, docx_path: str):\n",
        "        if not _HAS_DOCX:\n",
        "            raise RuntimeError(\"python-docx 미설치\")\n",
        "        if not os.path.exists(docx_path):\n",
        "            return None\n",
        "        try:\n",
        "            d = docx.Document(docx_path)\n",
        "        except Exception:\n",
        "            return None\n",
        "        paras = [p.text.strip() for p in d.paragraphs if p.text and p.text.strip()]\n",
        "        sections, cur_title, cur_buf = [], None, []\n",
        "        for p in d.paragraphs:\n",
        "            style = getattr(p.style, \"name\", \"\") or \"\"\n",
        "            text = (p.text or \"\").strip()\n",
        "            if not text:\n",
        "                continue\n",
        "            if style.startswith(\"Heading\") or \"제목\" in style:\n",
        "                if cur_title is not None or cur_buf:\n",
        "                    sections.append({\"title\": _norm(cur_title), \"text\": _norm(\"\\n\".join(cur_buf))})\n",
        "                cur_title, cur_buf = text, []\n",
        "            else:\n",
        "                cur_buf.append(text)\n",
        "        if cur_title is not None or cur_buf:\n",
        "            sections.append({\"title\": _norm(cur_title), \"text\": _norm(\"\\n\".join(cur_buf))})\n",
        "        full_text = _norm(\"\\n\".join(paras))\n",
        "        sentences = split_ko_sentences(full_text)\n",
        "        Doc = namedtuple(\"Doc\", [\"sections\", \"paragraphs\", \"sentences\", \"text\"])\n",
        "        return Doc(sections=sections, paragraphs=paras, sentences=sentences, text=full_text)\n",
        "\n",
        "# ── Section matching & evaluation ─────────────────────────────────────────────\n",
        "def find_best_section(\n",
        "    sections: List[Dict[str, str]],\n",
        "    required_title: str,\n",
        "    threshold: float = 0.4,\n",
        "    model_name: str = \"intfloat/e5-large\"\n",
        ") -> Optional[Dict[str, str]]:\n",
        "    if not sections:\n",
        "        return None\n",
        "\n",
        "    # 1) 임베딩 기반 (가능할 때)\n",
        "    if _HAS_SENTENCE_TRANSFORMERS and np is not None:\n",
        "        try:\n",
        "            model = _get_embedder(model_name)\n",
        "            titles = [s.get(\"title\", \"\") for s in sections]\n",
        "            emb_req = model.encode([required_title], normalize_embeddings=True)\n",
        "            emb_titles = model.encode(titles, normalize_embeddings=True)\n",
        "            sims = util.cos_sim(emb_req, emb_titles)[0].tolist()  # type: ignore\n",
        "            best_idx = int(np.argmax(sims))\n",
        "            best_sim = sims[best_idx]\n",
        "            if best_sim >= threshold:\n",
        "                return sections[best_idx]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2) 폴백: 키워드 유사도 최대값\n",
        "    best, best_score = None, -1.0\n",
        "    for s in sections:\n",
        "        sc = keyword_overlap(s.get(\"title\", \"\"), required_title)\n",
        "        if sc > best_score:\n",
        "            best, best_score = s, sc\n",
        "    return best if (best and best_score >= 0.15) else None  # 폴백 임계값 낮춤\n",
        "\n",
        "def evaluate_section(rt_title: str, sec_text: str) -> Dict[str, Any]:\n",
        "    sents = split_ko_sentences(sec_text)\n",
        "    if not (sec_text or \"\").strip():\n",
        "        return {\"required_title\": rt_title, \"exists\": False, \"final\": 0.0}\n",
        "    # 매우 단순한 길이 기반 정확도 근사 (향후 NLI로 대체 가능)\n",
        "    accuracy = 0.8 if len(sec_text) > 200 else 0.4\n",
        "    flu = _fluency(sents)\n",
        "    coh = simple_coherence(sents)\n",
        "    red_ngram = ngram_redundancy(sents, n=3)\n",
        "    red_cosine = cosine_redundancy(sents, model_name=\"intfloat/e5-large\", threshold=0.9)\n",
        "    redundancy = 0.5 * red_ngram + 0.5 * red_cosine\n",
        "    relevance = relevance_score(sec_text, rt_title)\n",
        "    consistency = consistency_score(sec_text)\n",
        "    final = (\n",
        "        0.25 * accuracy +\n",
        "        0.20 * relevance +\n",
        "        0.20 * coh +\n",
        "        0.15 * flu +\n",
        "        0.10 * consistency +\n",
        "        0.10 * (1 - redundancy)\n",
        "    )\n",
        "    return {\n",
        "        \"required_title\": rt_title, \"exists\": True, \"accuracy\": accuracy,\n",
        "        \"fluency\": flu, \"coherence\": coh, \"redundancy\": redundancy,\n",
        "        \"relevance\": relevance, \"consistency\": consistency, \"final\": final\n",
        "    }\n",
        "\n",
        "# ── Orchestrator ──────────────────────────────────────────────────────────────\n",
        "def run_combined_report(\n",
        "    docx_paths: List[str],\n",
        "    required_titles: List[str],\n",
        "    cfg: Optional[Dict[str, Any]],\n",
        "    out_path: str\n",
        "):\n",
        "    \"\"\"여러 DOCX를 대상 섹션 기준으로 평가 → Markdown/JSON/CSV 저장\"\"\"\n",
        "    results: Dict[str, List[Dict[str, Any]]] = defaultdict(list)\n",
        "    parser = DocParser() if _HAS_DOCX else None\n",
        "\n",
        "    for path in docx_paths:\n",
        "        if not parser:\n",
        "            # Docx 미설치 시 스킵\n",
        "            continue\n",
        "        doc = parser.parse(path)\n",
        "        if not doc:\n",
        "            continue\n",
        "        print(f\"[DEBUG] {os.path.basename(path)} 섹션 목록:\", [s[\"title\"] for s in doc.sections])\n",
        "\n",
        "        for rt in required_titles:\n",
        "            sec = find_best_section(doc.sections, rt, threshold=0.4)\n",
        "            text = sec[\"text\"] if sec else \"\"\n",
        "            res = evaluate_section(rt, text)\n",
        "            res[\"doc\"] = os.path.basename(path)\n",
        "            results[rt].append(res)\n",
        "\n",
        "    # 평균 요약\n",
        "    summary: Dict[str, Dict[str, float]] = {}\n",
        "    for rt, vals in results.items():\n",
        "        valid = [v for v in vals if v.get(\"exists\")]\n",
        "        if not valid:\n",
        "            continue\n",
        "        def avg(k): return sum(v[k] for v in valid) / len(valid)\n",
        "        summary[rt] = {\n",
        "            k: avg(k) for k in\n",
        "            [\"accuracy\", \"relevance\", \"coherence\", \"fluency\", \"consistency\", \"redundancy\", \"final\"]\n",
        "        }\n",
        "\n",
        "    # ── 저장물 생성 (Markdown / JSON / CSV)\n",
        "    md_lines = [\n",
        "        \"# 통합 문서 평가 보고서\\n\\n\",\n",
        "        \"| 제목 | Acc | Rel | Coh | Flu | Cons | Red(↓) | Final |\\n\",\n",
        "        \"|---|---:|---:|---:|---:|---:|---:|---:|\\n\"\n",
        "    ]\n",
        "    for rt in required_titles:\n",
        "        s = summary.get(rt)\n",
        "        if not s:\n",
        "            md_lines.append(f\"| {rt} | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\\n\")\n",
        "        else:\n",
        "            md_lines.append(\n",
        "                f\"| {rt} | {s['accuracy']:.2f} | {s['relevance']:.2f} | \"\n",
        "                f\"{s['coherence']:.2f} | {s['fluency']:.2f} | \"\n",
        "                f\"{s['consistency']:.2f} | {s['redundancy']:.2f} | {s['final']:.2f} |\\n\"\n",
        "            )\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\".join(md_lines))\n",
        "\n",
        "    base, _ = os.path.splitext(out_path)\n",
        "    json_path, csv_path = base + \"_summary.json\", base + \"_summary.csv\"\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "        json.dump(summary, jf, ensure_ascii=False, indent=2)\n",
        "\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        df = pd.DataFrame([{**{\"section\": k}, **v} for k, v in summary.items()])\n",
        "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "    except Exception:\n",
        "        csv_path = \"\"  # pandas 미설치 등\n",
        "\n",
        "    print(\" Markdown:\", out_path)\n",
        "    print(\" JSON:\", json_path)\n",
        "    if csv_path:\n",
        "        print(\" CSV :\", csv_path)\n",
        "    return out_path, json_path, csv_path"
      ]
    }
  ]
}